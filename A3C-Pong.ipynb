{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "# Code was runned using Docker on top of Ubuntu 22.04.1 LTS with Nvidia drivers 510. \n",
    "# Image: tensorflow/tensorflow:latest-gpu-jupyter\n",
    "# CUDA version: 11.6 \n",
    "# =======================================================================================================\n",
    "# Run with: \n",
    "#        docker pull tensorflow/tensorflow:latest-gpu-jupyter\n",
    "#        docker run --rm -it --gpus 0 --name=pong_tensorflow -p8888:8888 tensorflow/tensorflow:latest-gpu-jupyter\n",
    "# =======================================================================================================\n",
    "# In another shell:\n",
    "#        docker cp A3C-Pong.ipynb pong_tensorflow:/tf/\n",
    "# Accessing from browser, uncomment the code below to install all the proper software. \n",
    "# Re-comment it as soon as it finishes.\n",
    "# =======================================================================================================\n",
    "# Access output file with another shell:\n",
    "#        docker exec -it pong_tensorflow bash\n",
    "#        # tail -f Pong/output.txt\n",
    "# ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "#!apt-get update\n",
    "#!apt-get install -y xvfb ffmpeg cmake wget unrar python-opengl x11-utils\n",
    "#!apt-get upgrade --yes \n",
    "#\n",
    "#!pip install --upgrade pip\n",
    "#!pip install gym[atari] opencv-python atari-py pyglet pyvirtualdisplay ale-py lz4 gym-notebook-wrapper --upgrade\n",
    "# =======================================================================================================\n",
    "#!SHELL=/bin/bash\n",
    "#!if [ ! -f 'Roms.rar' ] ; then wget http://www.atarimania.com/roms/Roms.rar ; unrar e Roms.rar ; fi\n",
    "#!python -m atari_py.import_roms . \n",
    "# ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "import cv2\n",
    "import gym\n",
    "import gnwrapper\n",
    "import time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "# =======================================================================================================\n",
    "import tensorflow        as tf\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "# =======================================================================================================\n",
    "from time                import gmtime, strftime\n",
    "from collections         import deque\n",
    "from gym                 import envs\n",
    "from gym.wrappers        import FrameStack, GrayScaleObservation, ResizeObservation, AutoResetWrapper\n",
    "from tensorflow          import keras\n",
    "from keras               import backend\n",
    "from threading           import Thread, Lock\n",
    "from queue               import Queue\n",
    "from typing              import List, Tuple\n",
    "# ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "# tf.keras.backend.set_floatx('float32') # set keras default floating point precision type\n",
    "tf_fpp = tf.float32                      # tf floating point precision type \n",
    "np_fpp = np.float32                      # np floating point precision type\n",
    "tf.keras.backend.clear_session()\n",
    "# ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "maximum_number_of_workers = 64           # Single out-of-context variable to specify the maximum number of \n",
    "                                         # threads to run in parallel. This because of the creation of an array\n",
    "                                         # of environments in used in a **static** context in order to apply \n",
    "                                         # the @tf.function annotation and speed up the whole process.\n",
    "            \n",
    "                                         # Otherwise, calls to functions of non-static variables in an OO \n",
    "                                         # context would've required the *self* starting argument, not \n",
    "                                         # convertible to a Tensor, therefore not allowing the conversion to graph.\n",
    "# ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "# CropObservation wrapper crops the image(s) in input to a specified dimension. \n",
    "# =======================================================================================================\n",
    "class CropObservation(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, environment, kwargs):\n",
    "        super().__init__(environment)\n",
    "        self.x_low  = kwargs[\"x_low\"]\n",
    "        self.x_high = kwargs[\"x_high\"]\n",
    "        self.y_low  = kwargs[\"y_low\"]\n",
    "        self.y_high = kwargs[\"y_high\"]\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = observation[self.y_low:self.y_high, self.x_low:self.x_high]\n",
    "        return observation\n",
    "# =======================================================================================================\n",
    "\n",
    "# =======================================================================================================\n",
    "# DenoiseObservation wrapper uses computer vision's adaptive threshold function to highlight important features.\n",
    "# Original function documentation can be found here [https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html]\n",
    "# =======================================================================================================\n",
    "class DenoiseObservation(gym.ObservationWrapper):\n",
    "  \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        observation = cv2.adaptiveThreshold(observation, 255., cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 2)\n",
    "        return observation\n",
    "# =======================================================================================================\n",
    "  \n",
    "    \n",
    "# =======================================================================================================\n",
    "# NormalizeObservation wrapper divides the uint8 pixels by their max value (255.) in order to scale it to a value\n",
    "# between 0 and 1. This reduces the variance of values in the neural network.\n",
    "# =======================================================================================================\n",
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        observation = tf.cast(observation, dtype=tf.float32)\n",
    "        observation = tf.divide(observation, 255.)\n",
    "        return observation\n",
    "# =======================================================================================================\n",
    "\n",
    "# =======================================================================================================\n",
    "# Function that, based on the arguments passed in the last section, automatically wraps the environment \n",
    "# =======================================================================================================\n",
    "def make_env(kwargs):\n",
    "    \n",
    "    env = gym.make(env_name, render_mode=kwargs[\"render_mode\"], frameskip=kwargs[\"frameskip\"], \n",
    "             repeat_action_probability=kwargs[\"repeat_probability\"], difficulty=kwargs[\"difficulty\"])\n",
    "    env = AutoResetWrapper(env)\n",
    "    \n",
    "    if kwargs[\"crop\"]:\n",
    "        env = CropObservation(env, kwargs)\n",
    "    if kwargs[\"grayscale\"]:\n",
    "        env = GrayScaleObservation(env, keep_dim=True)\n",
    "    if kwargs[\"resize\"]:\n",
    "        env = ResizeObservation(env, (kwargs[\"screen_size\"], kwargs[\"screen_size\"]))\n",
    "    if kwargs[\"denoise\"]:\n",
    "        env = DenoiseObservation(env)\n",
    "    if kwargs[\"normalize\"]: \n",
    "        env = NormalizeObservation(env)\n",
    "    if kwargs[\"stack_frames\"]:\n",
    "        env = FrameStack(env, kwargs[\"n_frames\"])\n",
    "\n",
    "    return env\n",
    "# ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "# Actor-Critic model as specified in the original A3C paper in section 8 (Experimental Setup) of the \n",
    "# supplementary material.\n",
    "# A possible extension to the neural network would be to add LSTM cells to this architecture. \n",
    "# Another possible extension would be to directly use ConvLSTM2D layers offered by tensorflow.\n",
    "#\n",
    "# When experiencing fading gradients, it's adviced to switch the activation to leaky_relu.\n",
    "# \n",
    "# Technically, the actor layer's function activation should be a softmax. The reason to postpone the softmax \n",
    "# operation to later stages is intended: as reported in an older documentation, it should be more numerically \n",
    "# stable. Or at least it was. \n",
    "# [https://github.com/tensorflow/tensorflow/blob/4f50b5dc6426f63a8e70b65d3b9e55ed8f7d38e2/tensorflow/python/keras/losses.py#L437]\n",
    "# =======================================================================================================\n",
    "\n",
    "class ActorCriticModel(tf.keras.Model):\n",
    "    \n",
    "    def leaky_relu(self, x):\n",
    "        return tf.keras.activations.relu(x, alpha=self.alpha, max_value=None, threshold=0.0)\n",
    "        \n",
    "    def __init__(self, state_shape, action_size):\n",
    "\n",
    "        self.alpha = 1/16.0\n",
    "        \n",
    "        self.state_shape     = state_shape\n",
    "        self.action_size     = action_size\n",
    "\n",
    "        inputs           = tf.keras.layers.Input(shape=self.state_shape)\n",
    "        conv_1           = tf.keras.layers.Conv2D(8, 8, strides=2, activation='relu', kernel_initializer='he_uniform') (inputs)\n",
    "        conv_2           = tf.keras.layers.Conv2D(16, 4, strides=2, activation='relu', kernel_initializer='he_uniform') (conv_1)\n",
    "        conv_3           = tf.keras.layers.Conv2D(32, 2, strides=4, activation='relu', kernel_initializer='he_uniform') (conv_2)\n",
    "        flatten          = tf.keras.layers.Flatten() (conv_3)\n",
    "        dense_1          = tf.keras.layers.Dense(units=1024, activation=self.leaky_relu) (flatten)\n",
    "        critic           = tf.keras.layers.Dense(units=1) (dense_1)\n",
    "        actor            = tf.keras.layers.Dense(units=action_size) (dense_1) \n",
    "\n",
    "        super(ActorCriticModel, self).__init__(inputs=inputs, outputs=[actor, critic])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "# GlobalUpdater is the delegate that the asynchronous Worker uses to communicate with Controller.\n",
    "# It's called Global Updater because allows the update of the global model, held by Controller.\n",
    "# =======================================================================================================\n",
    "class GlobalUpdater():\n",
    "    \n",
    "    def __init__(self, controller_queue, worker_queue):\n",
    "        self.controller_queue = controller_queue\n",
    "        self.worker_queue = worker_queue\n",
    "    # =======================================================================================================            \n",
    "    # Operation to signal the controller the completion of an episode. It simply communicates all the variables \n",
    "    # needed to log the episode. \n",
    "    # =======================================================================================================\n",
    "    def episode_completion_update(self):\n",
    "        self.controller_queue.put((\"completion\", ()))\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Operation to signal the update of the global model. Gradients are sent through the queue to the Controller\n",
    "    # so that he can apply gradients using its optimizer.\n",
    "    # Returns: the updated global model weights to substitute the local model ones with.\n",
    "    # =======================================================================================================\n",
    "    def global_model_update(self, worker_idx, grads):\n",
    "        self.controller_queue.put((\"update\", (worker_idx, grads)))\n",
    "        return self.worker_queue.get()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Operation to request the global episode variable, held by Controller.\n",
    "    # =======================================================================================================\n",
    "    def get_global_episode(self, worker_idx):\n",
    "        self.controller_queue.put((\"get_episode\", (worker_idx,)))\n",
    "        return self.worker_queue.get()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Operation to request the weights of Controller's global model so to initialize every Worker's local model\n",
    "    # to have the correct initial weights.\n",
    "    # =======================================================================================================\n",
    "    def get_weights(self, worker_idx):\n",
    "        self.controller_queue.put((\"get_weights\", (worker_idx,)))\n",
    "        return self.worker_queue.get()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Operation to signal the Controller the reaching of the termination state of the worker. Termination state\n",
    "    # occurs when the global episode is larger than the maximum number of episodes to train the model\n",
    "    # =======================================================================================================            \n",
    "    def signal_termination(self):\n",
    "        self.controller_queue.put(None)\n",
    "    # =======================================================================================================\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================================\n",
    "# As specified above, this variable holds the static array accessed by Worker in a way that is compatible with\n",
    "# @tf.function annotation. This speeds up the training process by a lot.\n",
    "# =======================================================================================================\n",
    "\n",
    "envs = [None] * maximum_number_of_workers\n",
    "\n",
    "\n",
    "class Worker(Thread):\n",
    "    # =======================================================================================================\n",
    "    def __init__(self, worker_idx, updater, state_shape, kwargs):\n",
    "        super(Worker, self).__init__()\n",
    "        # Worker's identifier\n",
    "        self.worker_idx           = worker_idx\n",
    "        # State shape to initialize the ActorCriticModel class with\n",
    "        self.state_shape          = state_shape\n",
    "        # Updater to communicate with the Controller\n",
    "        self.updater              = updater\n",
    "        # Private Worker environment stored in a static array as specified above\n",
    "        # self.env                  = make_env(kwargs)\n",
    "        envs[self.worker_idx]     = make_env(kwargs)\n",
    "        # Name of the environment to explore\n",
    "        self.env_name             = kwargs[\"env_name\"]\n",
    "        # Set of available actions that the Worker can perform during its exploration\n",
    "        self.available_actions    = kwargs[\"available_actions\"]\n",
    "        self.action_size          = len(self.available_actions)\n",
    "        # Local ActorCriticModel used to explore and process gradients \n",
    "        self.local_model          = ActorCriticModel(self.state_shape, self.action_size)\n",
    "\n",
    "        \n",
    "        # Losses used to train the model. As mentioned above, both actor loss and entropy loss have the \n",
    "        # from_logits variable set to False for better stability\n",
    "        # Entropy loss can be calculated as the crossentropy with labels = true_values = probs\n",
    "        reduction                 = tf.keras.losses.Reduction.NONE\n",
    "        self.actor_loss_fn        = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=reduction)\n",
    "        self.critic_loss_fn       = tf.keras.losses.MeanSquaredError(reduction=reduction)\n",
    "        self.entropy_loss_fn      = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=reduction)\n",
    "        \n",
    "        # Hyperparams\n",
    "        self.max_eps              = kwargs[\"max_eps\"]\n",
    "        self.update_freq          = kwargs[\"update_freq\"]\n",
    "        self.gamma                = kwargs[\"gamma\"]\n",
    "        self.exponential_discount = kwargs[\"exponential_discount\"]\n",
    "        self.entropy_factor       = kwargs[\"entropy_factor\"]\n",
    "        self.critic_loss_factor   = kwargs[\"critic_loss_factor\"]\n",
    "        self.eps                  = kwargs[\"eps\"]\n",
    "        \n",
    "        print(f'Worker [{self.worker_idx}] started.')\n",
    "    # =======================================================================================================\n",
    "\n",
    "        \n",
    "    # =======================================================================================================\n",
    "    # Each of the following static methods is nothing else than a method that explicitly or implicitly use a \n",
    "    # Numpy function and its own tensorflow wrapper. This operation is being done to allow graph mode execution\n",
    "    # via the usage of @tf.function annotation.\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    @staticmethod\n",
    "    def env_reset(index):\n",
    "        new_state, info = envs[index].reset()\n",
    "        return np.array(new_state, np_fpp)\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_env_reset(index):\n",
    "        return tf.numpy_function(Worker.env_reset, [index], tf_fpp)\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    @staticmethod\n",
    "    def env_step(index, action):\n",
    "        new_state, reward, done, truncated, info = envs[index].step(action)\n",
    "        return (np.array(new_state, np_fpp), np.array(reward, np_fpp), np.array(done, np_fpp))\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_env_step(index, action):\n",
    "        return tf.numpy_function(Worker.env_step, [index, action], [tf_fpp, tf_fpp, tf_fpp])\n",
    "    # =======================================================================================================\n",
    "\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    @staticmethod\n",
    "    def get_action(available_actions, action_index):\n",
    "        return available_actions[action_index].astype(np.uint8)\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_get_action(available_actions, action_index):\n",
    "        return tf.numpy_function(Worker.get_action, [available_actions, action_index], tf.uint8)\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # This function explores the environment by playing for n steps. It stores every variable needed to \n",
    "    # calculate the gradient onto their own TensorArray, then proceeds to stack before passing them to the \n",
    "    # loss function. The 'set_shape' methods are used to preserve the variable shape across different \n",
    "    # iterations of the for loop.\n",
    "    #\n",
    "    # The bootstrapped value is calculated as specified in the paper:\n",
    "    # - 0        if state is a terminal state.\n",
    "    # - V(state) if state is not a terminal state.\n",
    "    #\n",
    "    # It's important to notice that this function is working while in a tf.GradientTape context. Therefore,\n",
    "    # the model is only called in this function and values are stored, so there's no need to call once more\n",
    "    # the model while in the function that calculates the total loss. \n",
    "    # =======================================================================================================\n",
    "    def play_steps(self, state):\n",
    "        \n",
    "        logits     = tf.TensorArray(dtype=tf_fpp, size=0, dynamic_size=True)\n",
    "        values     = tf.TensorArray(dtype=tf_fpp, size=0, dynamic_size=True)\n",
    "        rewards    = tf.TensorArray(dtype=tf_fpp, size=0, dynamic_size=True)\n",
    "        dones      = tf.TensorArray(dtype=tf_fpp, size=0, dynamic_size=True)\n",
    "        actions    = tf.TensorArray(dtype=tf.uint8, size=0, dynamic_size=True)\n",
    "        \n",
    "        for t in tf.range(self.update_freq):\n",
    "\n",
    "            logit, value        = self.local_model(state)\n",
    "            \n",
    "            action_index        = tf.cast(tf.squeeze(tf.random.categorical(logit, 1)), tf.uint8)\n",
    "            action              = Worker.tf_get_action(self.available_actions, action_index)\n",
    "            \n",
    "            state, reward, done = Worker.tf_env_step(self.worker_idx, action)\n",
    "\n",
    "            reward              = tf.convert_to_tensor(reward, dtype=tf_fpp)\n",
    "            state               = tf.expand_dims(tf.reshape(state, shape=self.state_shape), axis=0)\n",
    "            \n",
    "            logits              = logits.write(t, logit)\n",
    "            values              = values.write(t, value)\n",
    "            rewards             = rewards.write(t, reward)\n",
    "            dones               = dones.write(t, done)\n",
    "            actions             = actions.write(t, action_index)\n",
    "            \n",
    "        # Bootstrap last state\n",
    "        _, value = self.local_model(state)\n",
    "        values   = values.write(t + 1, value)\n",
    "            \n",
    "        return logits.stack(), values.stack(), rewards.stack(), dones.stack(), actions.stack(), state\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Function to process rewards and calculate the standard target values to update the value function and\n",
    "    # estimate advantages.\n",
    "    # =======================================================================================================\n",
    "    def get_expected_returns(self, values, dones, rewards):\n",
    "        \n",
    "        expected_return = values[-1]\n",
    "        expected_return_shape = expected_return.shape\n",
    "        expected_returns = tf.TensorArray(dtype=tf_fpp, size=0, dynamic_size=True)\n",
    "        \n",
    "        for t in tf.reverse(tf.range(self.update_freq), axis=[0]): \n",
    "            expected_return = (1.0 - dones[t]) * rewards[t] + self.gamma * expected_return\n",
    "            expected_return.set_shape(expected_return_shape)\n",
    "            expected_returns = expected_returns.write(t, expected_return)\n",
    "            \n",
    "        return tf.expand_dims(expected_returns.stack(), axis=-1)\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Function to process rewards and calculate Generalized Advantage Estimates as proposed in the \n",
    "    # \"High-Dimensional Continuous Control Using Generalized Advantage Estimation\" paper. Using this should \n",
    "    # improve learning with respect to the standard advantage estimation. \n",
    "    # =======================================================================================================\n",
    "    def get_generalized_advantage_estimate(self, values, dones, rewards): \n",
    "        \n",
    "        next_advantage   = tf.constant(0.0, dtype=tf_fpp)\n",
    "        next_adv_shape   = next_advantage.shape\n",
    "        advantages       = tf.TensorArray(dtype=tf_fpp, size=0, dynamic_size=True)\n",
    "        \n",
    "        values           = tf.squeeze(values, axis=1)\n",
    "        \n",
    "        next_values      = values[1:]\n",
    "        values           = values[:self.update_freq]\n",
    "        \n",
    "        for t in tf.reverse(tf.range(self.update_freq), axis=[0]):\n",
    "\n",
    "            delta          = rewards[t] + (self.gamma * (1.0 - dones[t]) * next_values[t]) - values[t]\n",
    "            next_advantage = delta + (self.gamma * self.exponential_discount * (1.0 - dones[t]) * next_advantage)\n",
    "            \n",
    "            next_advantage.set_shape(next_adv_shape)\n",
    "            advantages     = advantages.write(t, next_advantage)\n",
    "        \n",
    "        return advantages.stack()\n",
    "    # =======================================================================================================\n",
    "\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Computes the total loss of the trajectory experienced through the function play_steps. \n",
    "    # Depending on hyperparameters, one can either use standard advantage estimate or the general one. \n",
    "    # Furthermore, it's possible to clip rewards (to values [-1, 0, 1]) and to normalize advantages.\n",
    "    # The single loss functions used in this function do match the expected behavior:\n",
    "    # * actor   = - reduce_mean(reduce_sum(log(probs[action], axis=1))\n",
    "    # * entropy = - reduce_mean(reduce_sum(log(probs) * probs, axis=1))\n",
    "    # * critic  = reduce_mean((value_targets - values) ** 2)\n",
    "    #\n",
    "    # Since the gradient tape wraps the play_steps and the advantage estimation functions, it's mandatory to\n",
    "    # stop gradients earlier. Furthermore, the stop_gradient function has been applied to actions as well \n",
    "    # since the softmax_sparse_categorical_crossentropy_v2 function defined by tensorflow propagates gradients\n",
    "    # to true values as well and it's not clear whether this function is used in the SparseCategoricalCrossEntropy\n",
    "    # wrapper. \n",
    "    # ======================================================================================================= \n",
    "    def compute_loss(self, logits, values, rewards, dones, actions):\n",
    "        \n",
    "        logits         = tf.squeeze(logits, axis=1)\n",
    "        values         = tf.squeeze(values, axis=1)\n",
    "    \n",
    "        advantages     = self.get_generalized_advantage_estimate(values, dones, rewards)\n",
    "        value_targets  = self.get_expected_returns(values, dones, rewards)\n",
    "        \n",
    "        values         = values[:self.update_freq]\n",
    "        # value_targets  = tf.expand_dims(advantages, axis=1) + values\n",
    "        \n",
    "        actions        = tf.stop_gradient(actions)\n",
    "        advantages     = tf.stop_gradient(advantages)\n",
    "        actor_loss     = self.actor_loss_fn(y_true=actions, y_pred=logits, sample_weight=advantages)\n",
    "\n",
    "        value_targets  = tf.stop_gradient(value_targets)\n",
    "        critic_loss    = self.critic_loss_factor * self.critic_loss_fn(y_true=value_targets, y_pred=values) \n",
    "        \n",
    "        probs          = tf.nn.softmax(logits + self.eps)\n",
    "        entropy        = - self.entropy_factor * self.entropy_loss_fn(y_true=probs, y_pred=logits)  \n",
    "\n",
    "        total_loss     = tf.reduce_mean(actor_loss + entropy + critic_loss, axis=0)\n",
    "                \n",
    "        return total_loss\n",
    "    # =======================================================================================================\n",
    "\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Basic tf.function wrapper to encapsulate both play_steps and compute_loss function\n",
    "    # =======================================================================================================\n",
    "    @tf.function    \n",
    "    def step(self, state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            (logits, \n",
    "             values, \n",
    "             rewards, \n",
    "             dones,\n",
    "             actions, \n",
    "             state)    = self.play_steps(state)\n",
    "            \n",
    "            total_loss = self.compute_loss(logits, values, rewards, dones, actions)\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "            \n",
    "        if tf.reduce_sum(dones) > 0:\n",
    "            done = tf.constant(True, dtype=tf.bool)\n",
    "        else:\n",
    "            done = tf.constant(False, dtype=tf.bool)\n",
    "            \n",
    "        return state, done, grads\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Function that wraps all the behavior expected from the Worker. It initializes the states and \n",
    "    # communicates to Controller through GlobalUpdater both the gradients and the variables to be logged. \n",
    "    # =======================================================================================================\n",
    "    def run(self):\n",
    "        \n",
    "        # Initialize weights with global model ones\n",
    "        self.local_model.set_weights(self.updater.get_weights(self.worker_idx))\n",
    "        \n",
    "        state = tf.convert_to_tensor(Worker.env_reset(self.worker_idx), dtype=tf_fpp)\n",
    "        state = tf.expand_dims(tf.reshape(state, shape=self.state_shape), axis=0)\n",
    "        \n",
    "        self.episode = self.updater.get_global_episode(self.worker_idx)\n",
    "        \n",
    "        while self.episode < self.max_eps:\n",
    "            state, done, grads = self.step(state)\n",
    "\n",
    "            # Push local gradients to global model\n",
    "            self.local_model.set_weights(self.updater.global_model_update(self.worker_idx, grads))\n",
    "\n",
    "            if done: \n",
    "                # Signal completion to Controller\n",
    "                self.updater.episode_completion_update()\n",
    "                \n",
    "            self.episode = self.updater.get_global_episode(self.worker_idx)\n",
    "\n",
    "        self.updater.signal_termination()\n",
    "    # =======================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saver():\n",
    "    def __init__(self, save_dir, checkpoint):\n",
    "        # Directories and files where the progress is saved into\n",
    "        self.save_dir = save_dir\n",
    "        self.checkpoint = checkpoint\n",
    "        \n",
    "        self.output_file = save_dir + 'output.txt'\n",
    "        self.best_score_file = save_dir + 'best_score.bin'\n",
    "        self.episode_file = save_dir + 'episode.bin'\n",
    "        self.deque_file = save_dir + 'deque.bin'\n",
    "        self.plot_data_file = save_dir + 'plot_data.bin'\n",
    "        \n",
    "        self.cp_manager = tf.train.CheckpointManager(self.checkpoint, directory=save_dir, max_to_keep=2)\n",
    "        \n",
    "        self.lock = Lock()\n",
    "        self.output_lock = Lock()\n",
    "        \n",
    "    def restore_or_reset(self, restore, restore_from_best, moving_average_card):\n",
    "        if restore:\n",
    "            if restore_from_best:\n",
    "                self.checkpoint.restore(self.cp_manager.checkpoints[-2])\n",
    "            else:\n",
    "                self.checkpoint.restore(self.cp_manager.latest_checkpoint)\n",
    "            global_episode, best_score, rdeque, plot_data = self.load()\n",
    "            global_moving_average_reward = np.mean(rdeque)\n",
    "        else:\n",
    "            global_episode, best_score, rdeque, plot_data = 0, float('-inf'), deque(maxlen=moving_average_card), []\n",
    "            global_moving_average_reward = 0\n",
    "            self.reset()\n",
    "            \n",
    "        return global_episode, best_score, rdeque, plot_data, global_moving_average_reward\n",
    "        \n",
    "    def save(self, episode, best_score, deque, plot_data):\n",
    "        with self.lock:\n",
    "            print(f'Saving model to {self.save_dir}, best episode score: {best_score}')\n",
    "            self.cp_manager.save()\n",
    "            self.__store_element(self.episode_file, episode)\n",
    "            self.__store_element(self.best_score_file, best_score)\n",
    "            self.__store_element(self.deque_file, deque)\n",
    "            self.__store_element(self.plot_data_file, plot_data)\n",
    "            shutil.copy2(self.output_file, self.output_file + '.bak')\n",
    "\n",
    "    def load(self):\n",
    "        with self.lock:\n",
    "            episode = self.__load_element(self.episode_file)\n",
    "            best_score = self.__load_element(self.best_score_file)\n",
    "            deque = self.__load_element(self.deque_file)\n",
    "            plot_data = self.__load_element(self.plot_data_file)\n",
    "                 \n",
    "            self.__remove_output_file()\n",
    "            shutil.copy2(self.output_file + '.bak', self.output_file)\n",
    "            self.serialize_output(f' --- RESTORING EPISODE {episode} - BESTSCORE {best_score} ---')\n",
    "        \n",
    "        return episode, best_score, deque, plot_data\n",
    "    \n",
    "    def reset(self):\n",
    "        with self.lock:\n",
    "            self.__make_save_dir()\n",
    "            self.__remove_output_file()\n",
    "            \n",
    "    def serialize_output(self, output):\n",
    "        with self.output_lock:\n",
    "            self.__make_save_dir()\n",
    "            with open(self.output_file, \"a\") as file:\n",
    "                file.write(self.__timestamp(output) + \"\\n\"),\n",
    "            print(output)\n",
    "            \n",
    "    def __store_element(self, element_file, element):\n",
    "        self.__make_save_dir()\n",
    "        with open(element_file, \"wb\") as file:\n",
    "            pickle.dump(element, file)        \n",
    "        \n",
    "    def __load_element(self, element_file):\n",
    "        with open(element_file, \"rb\") as file:\n",
    "            element = pickle.load(file)\n",
    "        return element\n",
    "            \n",
    "    def __remove_output_file(self):\n",
    "        if os.path.exists(self.output_file):\n",
    "            os.remove(self.output_file)\n",
    "            print('Removed ' + self.output_file)\n",
    "            \n",
    "    def __make_save_dir(self):\n",
    "        if not os.path.isdir(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "    \n",
    "    def __timestamp(self, string):\n",
    "        return strftime(f'[%Y-%m-%d %H:%M:%S] {string}', gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Controller():\n",
    "    # =======================================================================================================\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Environment name\n",
    "        self.env_name                     = kwargs[\"env_name\"]\n",
    "        # List of available actions\n",
    "        self.available_actions            = kwargs[\"available_actions\"]\n",
    "        self.action_size                  = len(self.available_actions)\n",
    "        # Local environment for playing and displaying features\n",
    "        self.env                          = make_env(kwargs)\n",
    "        # Shape of the state used to initialize the ActorCriticModel neural network\n",
    "        self.state_shape                  = self.env.observation_space.shape\n",
    "        \n",
    "        # Global Model\n",
    "        self.global_model                 = ActorCriticModel(self.state_shape, self.action_size)\n",
    "        # Optimizer\n",
    "        self.opt                          = tf.keras.optimizers.Adam(\n",
    "                                                learning_rate=kwargs[\"lr_schedule\"], \n",
    "                                                global_clipnorm=kwargs[\"global_clipnorm\"], \n",
    "                                            )\n",
    "        # Cardinality of the moving average deque\n",
    "        self.moving_average_card          = kwargs[\"moving_average_card\"]\n",
    "        # Interval of episodes after which global model plays a game and prints output\n",
    "        self.play_interval                = kwargs[\"play_interval\"]\n",
    "        \n",
    "        # Checkpoint\n",
    "        self.checkpoint                   = tf.train.Checkpoint(optimizer=self.opt, model=self.global_model)\n",
    "\n",
    "        self.saver                        = Saver(kwargs[\"save_dir\"], tf.train.Checkpoint(optimizer=self.opt, model=self.global_model))\n",
    "        \n",
    "        (self.global_episode, \n",
    "        self.best_score, \n",
    "        self.deque, \n",
    "        self.plot_data, \n",
    "        self.global_moving_average_reward)= self.saver.restore_or_reset(kwargs[\"restore\"], kwargs[\"restore_from_best\"], self.moving_average_card)\n",
    "        \n",
    "        # Number of Workers\n",
    "        self.number_of_workers            = kwargs[\"number_of_workers\"]\n",
    "        # Pool of Worker threads\n",
    "        self.workers                      = []\n",
    "        # Queue used to communicate with Workers (Workers -> Controller)\n",
    "        self.controller_queue             = Queue()\n",
    "        # Queues used to communicate with Workers (Controller -> Worker)\n",
    "        self.worker_queues                = []\n",
    "        \n",
    "        # Initialization of threads\n",
    "        for i in range(self.number_of_workers):\n",
    "            worker_queue = Queue()\n",
    "            self.worker_queues.append(worker_queue)\n",
    "\n",
    "            updater = GlobalUpdater(self.controller_queue, worker_queue)\n",
    "            worker  = Worker(i, updater, self.state_shape, kwargs)\n",
    "            self.workers.append(worker)\n",
    "        \n",
    "    # =======================================================================================================\n",
    "            \n",
    "        \n",
    "    # =======================================================================================================\n",
    "    # Function to start the pool of threads and start working \n",
    "    # =======================================================================================================\n",
    "    def train(self, **kwargs):\n",
    "        \n",
    "        # Since we want the server to be online as soon as possible, each worker is started by a separate thread\n",
    "        # so that the Controller is ready to reply to early responses\n",
    "        Thread(target=self.start_workers, args=()).start()\n",
    "        self.terminated_workers = 0\n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.controller_queue.get()\n",
    "                if msg is not None:\n",
    "                    (task, args) = msg\n",
    "\n",
    "                    # Receive gradients from Worker and update the global model\n",
    "                    if task == \"update\":\n",
    "                        (worker_idx, grads) = args\n",
    "                        self.opt.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
    "                        self.worker_queues[worker_idx].put(self.global_model.get_weights())\n",
    "                    \n",
    "                    # Communicate weights to Worker to initialize local model weights\n",
    "                    if task == \"get_weights\":\n",
    "                        (worker_idx,) = args\n",
    "                        self.worker_queues[worker_idx].put(self.global_model.get_weights())\n",
    "                    \n",
    "                    # Communicate the current episode to Worker\n",
    "                    if task == \"get_episode\":\n",
    "                        (worker_idx,) = args\n",
    "                        self.worker_queues[worker_idx].put(self.global_episode)\n",
    "\n",
    "                    # Receive completion data from Worker\n",
    "                    if task == \"completion\":\n",
    "                        () = args\n",
    "\n",
    "                        # Update global episode\n",
    "                        self.global_episode += 1\n",
    "                        \n",
    "                        if self.global_episode % self.play_interval == 0:\n",
    "                            \n",
    "                            episode_reward, episode_steps, actions_count = self.global_model_play()\n",
    "                            # Update best score\n",
    "                            if episode_reward >= self.best_score and episode_reward > self.global_moving_average_reward:\n",
    "                                self.best_score = episode_reward\n",
    "                                self.saver.save(self.global_episode, self.best_score, self.deque, self.plot_data)\n",
    "                            # Update global moving average for rewards\n",
    "                            self.deque.append(episode_reward)\n",
    "                            # Compute moving average\n",
    "                            self.global_moving_average_reward = np.mean(self.deque)\n",
    "                            # Add result to list\n",
    "                            self.plot_data.append((self.global_moving_average_reward, episode_reward))\n",
    "                            # Serialize output\n",
    "                            self.saver.serialize_output(\n",
    "                                    f\"Episode: {self.global_episode:6} | \"\n",
    "                                    f\"MA{self.moving_average_card}: {self.global_moving_average_reward:+6.1f} | \"\n",
    "                                    f\"Episode Reward: {episode_reward:6} | \"\n",
    "                                    f\"Steps: {episode_steps:5} | \"\n",
    "                                    f\"Actions: {actions_count}\"\n",
    "                            )\n",
    "                    \n",
    "                else:\n",
    "                    self.terminated_workers = self.terminated_workers + 1\n",
    "                    if self.terminated_workers == self.number_of_workers:\n",
    "                        break\n",
    "\n",
    "        # When receiving the interruption signal, save state\n",
    "        except KeyboardInterrupt:\n",
    "            print(self.saver.serialize_output(\"Interrupting. Saving checkpoint...\"))\n",
    "            self.saver.save(self.global_episode, self.best_score, self.deque, self.plot_data)\n",
    "    # =======================================================================================================\n",
    "\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Debug function to properly show the images fed to the neural network\n",
    "    # =======================================================================================================    \n",
    "    def show_screen(self): \n",
    "        real_env        = gym.make(env_name)\n",
    "        print(self.env.observation_space.shape)\n",
    "\n",
    "        obs             = self.env.reset()\n",
    "        real_obs        = real_env.reset()\n",
    "\n",
    "        for _ in range(120):\n",
    "            obs, _, _, _, _      = self.env.step(0)\n",
    "            real_obs, _, _, _, _ = real_env.step(0)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Real Image')\n",
    "        plt.imshow(real_obs)\n",
    "        plt.figure()\n",
    "        plt.title('Edited Image 1')\n",
    "        plt.imshow(obs[0], cmap='gray')\n",
    "        plt.figure()\n",
    "        plt.title('Edited Image 2')\n",
    "        plt.imshow(obs[1], cmap='gray')\n",
    "        plt.figure()\n",
    "        plt.title('Edited Image 3')\n",
    "        plt.imshow(obs[2], cmap='gray')\n",
    "        plt.show()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Plot the average rewards graph\n",
    "    # =======================================================================================================\n",
    "    def plot(self): \n",
    "        \n",
    "        moving_average_rewards = []\n",
    "        rewards                = []\n",
    "        for data in self.plot_data:\n",
    "            moving_average_rewards.append(data[0])\n",
    "            rewards.append(data[1])\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(moving_average_rewards, 'r')\n",
    "        plt.plot(rewards, 'b')\n",
    "        plt.xlabel('Episode')\n",
    "        \n",
    "        plt.show()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Play the game with global model and record data\n",
    "    # =======================================================================================================\n",
    "    def global_model_play(self):\n",
    "        \n",
    "        done = False\n",
    "        state, _ = self.env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_steps  = 0\n",
    "        actions_count  = tf.Variable(tf.zeros(self.action_size))\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            state                      = tf.convert_to_tensor(state)\n",
    "            state                      = tf.expand_dims(tf.reshape(state, shape=self.state_shape), axis=0)\n",
    "            logit, _                   = self.global_model(state)\n",
    "            \n",
    "            action_index               = tf.cast(tf.squeeze(tf.random.categorical(logit, 1)), tf.uint8).numpy()\n",
    "            action                     = self.available_actions[action_index]\n",
    "            state, reward, done, _, _  = self.env.step(action)\n",
    "\n",
    "            action_index               = tf.convert_to_tensor(action_index, dtype=tf.uint8)\n",
    "            action_one_hot             = tf.one_hot(action, self.action_size)\n",
    "            actions_count.assign_add(action_one_hot)\n",
    "            \n",
    "            episode_reward            += reward\n",
    "            episode_steps             += 1\n",
    "        \n",
    "        return episode_reward, episode_steps, actions_count.numpy()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Play the game according to the neural network and render it\n",
    "    ################################################################\n",
    "    # GNWRAPPER 1.3.2 IS CURRENTLY BUGGED. DON'T USE THIS FUNCTION # \n",
    "    ################################################################\n",
    "    # =======================================================================================================\n",
    "    def play(self):\n",
    "        env   = gnwrapper.Animation(self.env)\n",
    "        state, _ = env.reset()\n",
    "        done  = False\n",
    "        \n",
    "        while not done:\n",
    "            state                = tf.convert_to_tensor(state)\n",
    "            state                = tf.expand_dims(tf.reshape(state, shape=self.state_shape), axis=0)\n",
    "            logits, _            = self.global_model(state)\n",
    "            logits               = tf.squeeze(logits)\n",
    "            probs                = tf.nn.softmax(logits)\n",
    "            \n",
    "            # No need to sample, just go for the best action\n",
    "            action_index         = tf.math.argmax(probs).numpy()\n",
    "            action               = self.available_actions[action_index]\n",
    "            \n",
    "            state, _, done, _, _ = env.step(action)\n",
    "            env.render()\n",
    "    # =======================================================================================================\n",
    "    \n",
    "    \n",
    "    # =======================================================================================================\n",
    "    # Accessory function to start Workers on a separate thread.\n",
    "    # A timer's set for further randomization: we might want to avoid situations where the global model\n",
    "    # is updated with almost simultaneous analogous updates. \n",
    "    # =======================================================================================================\n",
    "    def start_workers(self):\n",
    "        for worker in self.workers:\n",
    "            worker.start()\n",
    "            time.sleep(4)\n",
    "    # ======================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Actions: [0, 1, 2, 3, 4, 5]\n",
      "Selected Actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ale_py/roms/__init__.py:119: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  path = _resolve_rom(name)\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================================================\n",
    "# General Hyperparameters: here one can, after declaring an environment, put a constraints on the actions \n",
    "# to be used.\n",
    "# =======================================================================================================\n",
    "env_name          = 'ALE/Pong-v5'\n",
    "save_dir          = 'Pong/'\n",
    "max_eps           = 10_000\n",
    "temp_env          = gym.make(env_name)\n",
    "actions_meaning   = temp_env.unwrapped.get_action_meanings()\n",
    "possible_actions  = [i for i in range(temp_env.action_space.n)]\n",
    "available_actions = possible_actions\n",
    "selected_actions  = [actions_meaning[available_actions[i]] for i in range(len(available_actions))]\n",
    "print(f\"Possible Actions: {possible_actions}\")\n",
    "print(f\"Selected Actions: {selected_actions}\")\n",
    "# =======================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Pong/output.txt\n",
      "Worker [0] started.\n",
      "Worker [1] started.\n",
      "Worker [2] started.\n",
      "Worker [3] started.\n",
      "Worker [4] started.\n",
      "Worker [5] started.\n",
      "Worker [6] started.\n",
      "Worker [7] started.\n",
      "Worker [8] started.\n",
      "Worker [9] started.\n",
      "Worker [10] started.\n",
      "Worker [11] started.\n",
      "Worker [12] started.\n",
      "Worker [13] started.\n",
      "Worker [14] started.\n",
      "Worker [15] started.\n"
     ]
    }
   ],
   "source": [
    "# Controller initialization with hyperparameters\n",
    "# =======================================================================================================\n",
    "agent = Controller(\n",
    "    # Environment name\n",
    "    env_name             = env_name,\n",
    "    # Directory where progress is saved\n",
    "    save_dir             = save_dir,\n",
    "    # List of available actions\n",
    "    available_actions    = available_actions,\n",
    "    # Cardinality of the set upon which the average is computed\n",
    "    moving_average_card  = 50,\n",
    "    # Number of threads\n",
    "    number_of_workers    = 16,\n",
    "# =======================================================================================================\n",
    "    # Render mode for environment [rgb_array/human]. Don't edit.\n",
    "    render_mode          = \"rgb_array\", \n",
    "    # Number of frames to be skipped, repeating the action selected\n",
    "    frameskip            = 4, \n",
    "    # Probability of repeating the previous action\n",
    "    repeat_probability   = 0.0, \n",
    "    # Difficulty of the game [check: https://www.gymlibrary.dev/environments/atari/complete_list/]\n",
    "    difficulty           = 1\n",
    "# =======================================================================================================\n",
    "    # Learning Rate\n",
    "    lr_schedule          = 5e-4,\n",
    "    # Global Clipnorm\n",
    "    global_clipnorm      = 1,\n",
    "# =======================================================================================================\n",
    "    # Max length for training\n",
    "    max_eps              = max_eps,\n",
    "    # Controller plays and outputs every *play_interval* games\n",
    "    play_interval        = 10,\n",
    "    # Frequency of neural network updates\n",
    "    update_freq          = 32,\n",
    "    # Gamma\n",
    "    gamma                = 0.99,\n",
    "    # Lambda - GAE\n",
    "    exponential_discount = 0.95,\n",
    "    # Entropy factor\n",
    "    entropy_factor       = 0.005,\n",
    "    # Critic loss multiplier\n",
    "    critic_loss_factor   = 0.5,\n",
    "    # Epsilon a variable related to numerical stability in optimizer and normalization computations\n",
    "    # It's useful, for example, to avoid division by zeros or fading entropy (lim (x->0) x * log(x) = 0)\n",
    "    eps                  = 1e-7,\n",
    "# =======================================================================================================\n",
    "    # Grayscale images\n",
    "    grayscale            = True,\n",
    "    # Clip rewards to [-1, 0, 1]\n",
    "    clip_rewards         = True,\n",
    "    # Divide images input fed to neural networks for 255 so to have values in [0,1]\n",
    "    normalize            = True,\n",
    "    # Crop image to specified values\n",
    "    crop                 = True,      # value function needs something to distinguish states -> scores\n",
    "    x_low                = 0,         # default = 0\n",
    "    x_high               = 160,       # default = 160\n",
    "    y_low                = 34,        # default = 0\n",
    "    y_high               = 194,       # default = 210\n",
    "    # Resize screen to specified value\n",
    "    resize               = True,\n",
    "    screen_size          = 84,\n",
    "    # Denoise images\n",
    "    denoise              = True,\n",
    "    # Stack states so to have continuity and let neural network understand directions\n",
    "    stack_frames         = True,\n",
    "    # Number of frames to stack\n",
    "    n_frames             = 4,\n",
    "# =======================================================================================================\n",
    "    # Restore saved progress\n",
    "    restore              = False,\n",
    "    # Either restore from best (True) or latest (False)\n",
    "    restore_from_best    = False, \n",
    "# =======================================================================================================\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.show_screen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f3de1b4cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f3de1b4cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f3de1b4cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f3de1b4cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     10 | MA50:  -21.0 | Episode Reward:  -21.0 | Steps:   764 | Actions: [  5.  37. 646.   0.  63.  13.]\n",
      "Episode:     20 | MA50:  -21.0 | Episode Reward:  -21.0 | Steps:   764 | Actions: [ 44. 237. 240.  52. 116.  75.]\n",
      "Episode:     30 | MA50:  -21.0 | Episode Reward:  -21.0 | Steps:   764 | Actions: [  5. 488.  23.  15. 182.  51.]\n",
      "Episode:     40 | MA50:  -21.0 | Episode Reward:  -21.0 | Steps:   811 | Actions: [ 28. 126.  39. 517.  27.  74.]\n",
      "Episode:     50 | MA50:  -21.0 | Episode Reward:  -21.0 | Steps:   792 | Actions: [117. 161. 100. 195.  87. 132.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:     60 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   930 | Actions: [224. 199. 153. 310.  22.  22.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:     70 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:   885 | Actions: [ 90. 160. 125. 192. 126. 192.]\n",
      "Episode:     80 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   764 | Actions: [ 60. 146. 239.  62. 159.  98.]\n",
      "Episode:     90 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   792 | Actions: [ 38. 141. 107. 247. 176.  83.]\n",
      "Episode:    100 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   783 | Actions: [  3.  16.  10. 270. 144. 340.]\n",
      "Episode:    110 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   914 | Actions: [ 77.  91.  34. 305. 230. 177.]\n",
      "Episode:    120 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   764 | Actions: [166.  83.  88. 112. 225.  90.]\n",
      "Episode:    130 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:  1125 | Actions: [480.  72. 206. 106.  82. 179.]\n",
      "Episode:    140 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   976 | Actions: [162. 113. 151. 230. 159. 161.]\n",
      "Episode:    150 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   842 | Actions: [ 89.  32. 189. 226.  56. 250.]\n",
      "Episode:    160 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   852 | Actions: [127.  15. 251. 176.  57. 226.]\n",
      "Episode:    170 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   888 | Actions: [143.  20. 106. 350.  46. 223.]\n",
      "Episode:    180 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   852 | Actions: [331. 167. 123. 103.  61.  67.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    190 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   842 | Actions: [167.  57. 119. 214.  74. 211.]\n",
      "Episode:    200 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   783 | Actions: [ 88.  37.  50. 269.  23. 316.]\n",
      "Episode:    210 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   884 | Actions: [132.  40.  56. 150. 110. 396.]\n",
      "Episode:    220 | MA50:  -20.9 | Episode Reward:  -21.0 | Steps:   826 | Actions: [ 98.  59.  98. 160.  94. 317.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    230 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   842 | Actions: [118.  78. 128. 239. 121. 158.]\n",
      "Episode:    240 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   852 | Actions: [ 99. 175.  79. 257. 106. 136.]\n",
      "Episode:    250 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   826 | Actions: [ 74.  57.  81. 328.  82. 204.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    260 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   948 | Actions: [183. 139. 113. 170. 133. 210.]\n",
      "Episode:    270 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   963 | Actions: [192. 108. 187. 105. 101. 270.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    280 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   838 | Actions: [144. 124.  84. 208.  82. 196.]\n",
      "Episode:    290 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   916 | Actions: [ 95. 145. 117. 227. 105. 227.]\n",
      "Episode:    300 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   852 | Actions: [108.  90. 108. 129. 188. 229.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    310 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   915 | Actions: [ 67. 133.  83. 156. 210. 266.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    320 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   934 | Actions: [168. 116. 115. 169. 181. 185.]\n",
      "Episode:    330 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   936 | Actions: [193. 126.  65. 133. 267. 152.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    340 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:   931 | Actions: [180. 208. 119. 153. 159. 112.]\n",
      "Episode:    350 | MA50:  -20.7 | Episode Reward:  -21.0 | Steps:   824 | Actions: [ 80. 104.  73. 223. 155. 189.]\n",
      "Episode:    360 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   783 | Actions: [132. 162.  63. 171.  93. 162.]\n",
      "Episode:    370 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   783 | Actions: [134.  98.  47. 310.  98.  96.]\n",
      "Episode:    380 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   764 | Actions: [736.   0.   0.  28.   0.   0.]\n",
      "Episode:    390 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:   885 | Actions: [189. 193.  37. 276. 110.  80.]\n",
      "Episode:    400 | MA50:  -20.8 | Episode Reward:  -21.0 | Steps:  1006 | Actions: [176. 129.  92. 312. 137. 160.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    410 | MA50:  -20.8 | Episode Reward:  -20.0 | Steps:   866 | Actions: [128.  76. 104. 151. 169. 238.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    420 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:  1041 | Actions: [269. 139. 130. 168. 161. 174.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    430 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:   861 | Actions: [176. 144. 104. 180. 140. 117.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    440 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:  1307 | Actions: [140. 207. 310. 230. 166. 254.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    450 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:  1446 | Actions: [213. 152. 286. 307. 221. 267.]\n",
      "Saving model to Pong/, best episode score: -20.0\n",
      "Episode:    460 | MA50:  -20.7 | Episode Reward:  -20.0 | Steps:  1296 | Actions: [210. 219. 137. 226. 250. 254.]\n",
      "Saving model to Pong/, best episode score: -19.0\n",
      "Episode:    470 | MA50:  -20.6 | Episode Reward:  -19.0 | Steps:  1196 | Actions: [327. 129. 179. 141. 233. 187.]\n",
      "Episode:    480 | MA50:  -20.6 | Episode Reward:  -20.0 | Steps:  1199 | Actions: [295.  90. 239. 185.  88. 302.]\n",
      "Episode:    490 | MA50:  -20.6 | Episode Reward:  -20.0 | Steps:   885 | Actions: [210.  73. 132.  84. 205. 181.]\n",
      "Episode:    500 | MA50:  -20.6 | Episode Reward:  -21.0 | Steps:   764 | Actions: [206.  53.  27.  64. 347.  67.]\n",
      "Episode:    510 | MA50:  -20.6 | Episode Reward:  -20.0 | Steps:  1729 | Actions: [312. 195. 348. 356. 246. 272.]\n",
      "Saving model to Pong/, best episode score: -14.0\n",
      "Episode:    520 | MA50:  -20.5 | Episode Reward:  -14.0 | Steps:  1979 | Actions: [357. 161. 313. 466. 273. 409.]\n",
      "Episode:    530 | MA50:  -20.4 | Episode Reward:  -18.0 | Steps:  1903 | Actions: [253. 140. 306. 476. 243. 485.]\n",
      "Episode:    540 | MA50:  -20.3 | Episode Reward:  -18.0 | Steps:  1810 | Actions: [281. 221. 344. 442. 295. 227.]\n",
      "Episode:    550 | MA50:  -20.2 | Episode Reward:  -16.0 | Steps:  2042 | Actions: [336. 237. 313. 438. 361. 357.]\n",
      "Episode:    560 | MA50:  -20.2 | Episode Reward:  -19.0 | Steps:  2048 | Actions: [355. 235. 398. 407. 314. 339.]\n",
      "Episode:    570 | MA50:  -20.2 | Episode Reward:  -20.0 | Steps:  1829 | Actions: [333. 172. 395. 342. 278. 309.]\n",
      "Episode:    580 | MA50:  -20.1 | Episode Reward:  -16.0 | Steps:  2064 | Actions: [347. 165. 454. 353. 421. 324.]\n",
      "Saving model to Pong/, best episode score: -14.0\n",
      "Episode:    590 | MA50:  -20.0 | Episode Reward:  -14.0 | Steps:  2478 | Actions: [450. 267. 469. 496. 353. 443.]\n",
      "Episode:    600 | MA50:  -19.9 | Episode Reward:  -18.0 | Steps:  2339 | Actions: [357. 284. 437. 448. 397. 416.]\n",
      "Saving model to Pong/, best episode score: -12.0\n",
      "Episode:    610 | MA50:  -19.7 | Episode Reward:  -12.0 | Steps:  2593 | Actions: [411. 299. 428. 447. 548. 460.]\n",
      "Episode:    620 | MA50:  -19.6 | Episode Reward:  -16.0 | Steps:  1574 | Actions: [311. 180. 309. 240. 316. 218.]\n",
      "Saving model to Pong/, best episode score: -7.0\n",
      "Episode:    630 | MA50:  -19.4 | Episode Reward:   -7.0 | Steps:  2679 | Actions: [395. 286. 350. 565. 715. 368.]\n",
      "Episode:    640 | MA50:  -19.1 | Episode Reward:   -9.0 | Steps:  2878 | Actions: [537. 268. 584. 622. 456. 411.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:    660 | MA50:  -18.5 | Episode Reward:   -9.0 | Steps:  2880 | Actions: [441. 373. 690. 479. 403. 494.]\n",
      "Episode:    670 | MA50:  -18.2 | Episode Reward:   -7.0 | Steps:  3562 | Actions: [465. 534. 773. 818. 450. 522.]\n",
      "Episode:    680 | MA50:  -18.0 | Episode Reward:   -9.0 | Steps:  2974 | Actions: [474. 390. 629. 619. 473. 389.]\n",
      "Episode:    690 | MA50:  -18.0 | Episode Reward:  -20.0 | Steps:  1015 | Actions: [168. 313.  57. 241. 153.  83.]\n",
      "Episode:    700 | MA50:  -17.7 | Episode Reward:   -6.0 | Steps:  2849 | Actions: [494. 580. 454. 538. 442. 341.]\n",
      "Saving model to Pong/, best episode score: 13.0\n",
      "Episode:    710 | MA50:  -17.0 | Episode Reward:   13.0 | Steps:  2675 | Actions: [395. 350. 483. 663. 457. 327.]\n",
      "Saving model to Pong/, best episode score: 13.0\n",
      "Episode:    720 | MA50:  -16.3 | Episode Reward:   13.0 | Steps:  3089 | Actions: [368. 440. 587. 848. 490. 356.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    730 | MA50:  -15.5 | Episode Reward:   20.0 | Steps:  2128 | Actions: [317. 286. 353. 533. 355. 284.]\n",
      "Episode:    740 | MA50:  -15.3 | Episode Reward:   -7.0 | Steps:  4357 | Actions: [ 679.  570.  719. 1091.  629.  669.]\n",
      "Episode:    750 | MA50:  -14.6 | Episode Reward:   10.0 | Steps:  3146 | Actions: [408. 516. 523. 813. 503. 383.]\n",
      "Episode:    760 | MA50:  -14.0 | Episode Reward:   14.0 | Steps:  3152 | Actions: [447. 454. 547. 759. 387. 558.]\n",
      "Episode:    770 | MA50:  -13.2 | Episode Reward:   16.0 | Steps:  2254 | Actions: [299. 325. 402. 446. 341. 441.]\n",
      "Episode:    780 | MA50:  -12.5 | Episode Reward:   17.0 | Steps:  2564 | Actions: [286. 319. 555. 644. 346. 414.]\n",
      "Episode:    790 | MA50:  -11.7 | Episode Reward:   16.0 | Steps:  2278 | Actions: [339. 272. 453. 421. 391. 402.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    800 | MA50:  -10.9 | Episode Reward:   20.0 | Steps:  2068 | Actions: [366. 253. 349. 567. 195. 338.]\n",
      "Episode:    810 | MA50:  -10.2 | Episode Reward:   16.0 | Steps:  2293 | Actions: [332. 331. 454. 543. 268. 365.]\n",
      "Episode:    820 | MA50:   -9.9 | Episode Reward:   -4.0 | Steps:  2643 | Actions: [357. 386. 703. 402. 342. 453.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    830 | MA50:   -9.1 | Episode Reward:   20.0 | Steps:  2068 | Actions: [278. 317. 428. 439. 198. 408.]\n",
      "Episode:    840 | MA50:   -8.3 | Episode Reward:   18.0 | Steps:  2325 | Actions: [309. 408. 403. 458. 247. 500.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    850 | MA50:   -7.5 | Episode Reward:   20.0 | Steps:  2008 | Actions: [230. 311. 329. 443. 247. 448.]\n",
      "Episode:    860 | MA50:   -6.7 | Episode Reward:   16.0 | Steps:  2249 | Actions: [309. 331. 303. 492. 345. 469.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    870 | MA50:   -5.9 | Episode Reward:   20.0 | Steps:  2096 | Actions: [281. 425. 248. 524. 210. 408.]\n",
      "Episode:    880 | MA50:   -5.1 | Episode Reward:   18.0 | Steps:  2202 | Actions: [303. 403. 260. 566. 287. 383.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    890 | MA50:   -4.3 | Episode Reward:   20.0 | Steps:  2068 | Actions: [292. 363. 235. 599. 239. 340.]\n",
      "Episode:    900 | MA50:   -3.7 | Episode Reward:    9.0 | Steps:  2787 | Actions: [388. 384. 478. 640. 441. 456.]\n",
      "Episode:    910 | MA50:   -2.9 | Episode Reward:   19.0 | Steps:  2104 | Actions: [253. 310. 294. 523. 341. 383.]\n",
      "Episode:    920 | MA50:   -2.2 | Episode Reward:   17.0 | Steps:  2254 | Actions: [256. 286. 276. 492. 345. 599.]\n",
      "Episode:    930 | MA50:   -1.5 | Episode Reward:   16.0 | Steps:  2413 | Actions: [299. 312. 267. 521. 337. 677.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    940 | MA50:   -0.7 | Episode Reward:   20.0 | Steps:  2068 | Actions: [336. 240. 276. 474. 255. 487.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    950 | MA50:   +0.1 | Episode Reward:   20.0 | Steps:  2095 | Actions: [302. 300. 320. 473. 233. 467.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    960 | MA50:   +0.9 | Episode Reward:   20.0 | Steps:  2068 | Actions: [343. 294. 252. 442. 249. 488.]\n",
      "Episode:    970 | MA50:   +1.6 | Episode Reward:   14.0 | Steps:  2476 | Actions: [376. 376. 300. 496. 402. 526.]\n",
      "Episode:    980 | MA50:   +2.3 | Episode Reward:   17.0 | Steps:  2275 | Actions: [273. 412. 304. 415. 294. 577.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:    990 | MA50:   +3.1 | Episode Reward:   20.0 | Steps:  2068 | Actions: [294. 334. 266. 429. 243. 502.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1000 | MA50:   +3.9 | Episode Reward:   20.0 | Steps:  2068 | Actions: [323. 366. 226. 450. 249. 454.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1010 | MA50:   +4.7 | Episode Reward:   20.0 | Steps:  2068 | Actions: [362. 305. 242. 597. 283. 279.]\n",
      "Episode:   1020 | MA50:   +5.3 | Episode Reward:   16.0 | Steps:  2239 | Actions: [322. 324. 355. 590. 344. 304.]\n",
      "Episode:   1030 | MA50:   +6.1 | Episode Reward:   18.0 | Steps:  2160 | Actions: [408. 323. 363. 458. 228. 380.]\n",
      "Episode:   1040 | MA50:   +6.8 | Episode Reward:   18.0 | Steps:  2215 | Actions: [387. 334. 346. 511. 205. 432.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1050 | MA50:   +7.5 | Episode Reward:   20.0 | Steps:  2068 | Actions: [385. 340. 315. 453. 176. 399.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1060 | MA50:   +8.3 | Episode Reward:   20.0 | Steps:  2068 | Actions: [346. 326. 285. 455. 206. 450.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1070 | MA50:   +9.1 | Episode Reward:   20.0 | Steps:  2068 | Actions: [221. 287. 272. 560. 194. 534.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1080 | MA50:   +9.8 | Episode Reward:   20.0 | Steps:  2068 | Actions: [260. 342. 231. 476. 238. 521.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1090 | MA50:  +10.5 | Episode Reward:   20.0 | Steps:  2128 | Actions: [332. 425. 269. 433. 285. 384.]\n",
      "Episode:   1100 | MA50:  +11.2 | Episode Reward:   17.0 | Steps:  2181 | Actions: [420. 345. 344. 384. 263. 425.]\n",
      "Episode:   1110 | MA50:  +11.7 | Episode Reward:   16.0 | Steps:  2252 | Actions: [414. 312. 279. 452. 281. 514.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1120 | MA50:  +12.5 | Episode Reward:   20.0 | Steps:  2068 | Actions: [377. 331. 224. 361. 274. 501.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1130 | MA50:  +13.0 | Episode Reward:   20.0 | Steps:  2068 | Actions: [277. 347. 246. 383. 282. 533.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1140 | MA50:  +13.6 | Episode Reward:   20.0 | Steps:  2096 | Actions: [348. 339. 288. 329. 285. 507.]\n",
      "Episode:   1150 | MA50:  +14.0 | Episode Reward:   19.0 | Steps:  2044 | Actions: [376. 322. 289. 322. 256. 479.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1160 | MA50:  +14.6 | Episode Reward:   20.0 | Steps:  2068 | Actions: [368. 316. 342. 343. 168. 531.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1170 | MA50:  +15.1 | Episode Reward:   20.0 | Steps:  2008 | Actions: [371. 258. 327. 357. 205. 490.]\n",
      "Episode:   1180 | MA50:  +15.7 | Episode Reward:   18.0 | Steps:  2366 | Actions: [371. 295. 410. 499. 230. 561.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1190 | MA50:  +16.5 | Episode Reward:   20.0 | Steps:  2096 | Actions: [328. 292. 330. 486. 179. 481.]\n",
      "Episode:   1200 | MA50:  +16.7 | Episode Reward:    6.0 | Steps:  2982 | Actions: [474. 544. 651. 532. 298. 483.]\n",
      "Episode:   1210 | MA50:  +16.8 | Episode Reward:   17.0 | Steps:  2318 | Actions: [401. 383. 452. 443. 200. 439.]\n",
      "Episode:   1220 | MA50:  +16.9 | Episode Reward:   19.0 | Steps:  2314 | Actions: [435. 343. 352. 452. 243. 489.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1230 | MA50:  +16.9 | Episode Reward:   20.0 | Steps:  2068 | Actions: [320. 332. 272. 534. 207. 403.]\n",
      "Episode:   1240 | MA50:  +17.4 | Episode Reward:   19.0 | Steps:  2104 | Actions: [350. 305. 300. 448. 220. 481.]\n",
      "Episode:   1250 | MA50:  +17.5 | Episode Reward:   15.0 | Steps:  2479 | Actions: [351. 455. 340. 486. 362. 485.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1260 | MA50:  +17.7 | Episode Reward:   20.0 | Steps:  2008 | Actions: [310. 361. 229. 394. 240. 474.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   1270 | MA50:  +17.6 | Episode Reward:   15.0 | Steps:  2441 | Actions: [394. 375. 314. 505. 337. 516.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1280 | MA50:  +17.7 | Episode Reward:   20.0 | Steps:  2008 | Actions: [306. 299. 269. 426. 248. 460.]\n",
      "Episode:   1290 | MA50:  +17.7 | Episode Reward:   18.0 | Steps:  2200 | Actions: [292. 352. 447. 420. 233. 456.]\n",
      "Episode:   1300 | MA50:  +17.7 | Episode Reward:   18.0 | Steps:  2189 | Actions: [306. 294. 366. 418. 269. 536.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1310 | MA50:  +17.8 | Episode Reward:   20.0 | Steps:  2068 | Actions: [221. 338. 271. 564. 268. 406.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1320 | MA50:  +18.3 | Episode Reward:   20.0 | Steps:  2068 | Actions: [226. 351. 294. 479. 259. 459.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1330 | MA50:  +18.3 | Episode Reward:   20.0 | Steps:  2068 | Actions: [315. 276. 271. 526. 272. 408.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1340 | MA50:  +18.3 | Episode Reward:   20.0 | Steps:  2128 | Actions: [269. 328. 259. 523. 248. 501.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1350 | MA50:  +18.3 | Episode Reward:   20.0 | Steps:  2096 | Actions: [220. 333. 346. 438. 233. 526.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1360 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2162 | Actions: [296. 366. 243. 409. 241. 607.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1370 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2036 | Actions: [361. 310. 250. 362. 288. 465.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1380 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2128 | Actions: [376. 412. 215. 452. 233. 440.]\n",
      "Episode:   1390 | MA50:  +18.2 | Episode Reward:    7.0 | Steps:  2869 | Actions: [330. 528. 377. 533. 375. 726.]\n",
      "Episode:   1400 | MA50:  +18.3 | Episode Reward:   16.0 | Steps:  2272 | Actions: [368. 360. 394. 417. 299. 434.]\n",
      "Episode:   1410 | MA50:  +18.3 | Episode Reward:   19.0 | Steps:  2044 | Actions: [325. 320. 392. 356. 202. 449.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1420 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2093 | Actions: [336. 351. 357. 321. 228. 500.]\n",
      "Episode:   1430 | MA50:  +18.4 | Episode Reward:   19.0 | Steps:  2065 | Actions: [331. 359. 311. 439. 243. 382.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1440 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2068 | Actions: [324. 348. 314. 377. 254. 451.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1450 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2142 | Actions: [286. 325. 348. 473. 253. 457.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1460 | MA50:  +18.4 | Episode Reward:   20.0 | Steps:  2068 | Actions: [274. 290. 308. 447. 290. 459.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1470 | MA50:  +18.5 | Episode Reward:   20.0 | Steps:  2087 | Actions: [320. 315. 322. 468. 255. 407.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1480 | MA50:  +18.6 | Episode Reward:   20.0 | Steps:  2068 | Actions: [280. 306. 352. 425. 231. 474.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1490 | MA50:  +18.6 | Episode Reward:   20.0 | Steps:  2068 | Actions: [220. 285. 349. 542. 264. 408.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1500 | MA50:  +18.6 | Episode Reward:   20.0 | Steps:  2096 | Actions: [234. 313. 301. 505. 283. 460.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1510 | MA50:  +18.6 | Episode Reward:   20.0 | Steps:  2068 | Actions: [232. 274. 310. 454. 338. 460.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1520 | MA50:  +18.7 | Episode Reward:   20.0 | Steps:  2128 | Actions: [329. 269. 263. 509. 301. 457.]\n",
      "Episode:   1530 | MA50:  +18.7 | Episode Reward:   18.0 | Steps:  2316 | Actions: [323. 317. 296. 456. 347. 577.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1540 | MA50:  +18.7 | Episode Reward:   20.0 | Steps:  2008 | Actions: [261. 307. 259. 480. 277. 424.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1550 | MA50:  +18.7 | Episode Reward:   20.0 | Steps:  2068 | Actions: [324. 354. 312. 354. 204. 520.]\n",
      "Episode:   1560 | MA50:  +18.7 | Episode Reward:   19.0 | Steps:  2329 | Actions: [388. 344. 356. 473. 248. 520.]\n",
      "Episode:   1570 | MA50:  +18.7 | Episode Reward:   19.0 | Steps:  2226 | Actions: [329. 276. 334. 427. 330. 530.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1580 | MA50:  +18.7 | Episode Reward:   20.0 | Steps:  2167 | Actions: [301. 307. 340. 464. 291. 464.]\n",
      "Episode:   1590 | MA50:  +18.7 | Episode Reward:   19.0 | Steps:  2225 | Actions: [343. 363. 332. 458. 231. 498.]\n",
      "Episode:   1600 | MA50:  +18.7 | Episode Reward:   18.0 | Steps:  2168 | Actions: [326. 330. 354. 395. 279. 484.]\n",
      "Episode:   1610 | MA50:  +18.7 | Episode Reward:   18.0 | Steps:  2162 | Actions: [336. 346. 373. 368. 308. 431.]\n",
      "Saving model to Pong/, best episode score: 20.0\n",
      "Episode:   1620 | MA50:  +18.7 | Episode Reward:   20.0 | Steps:  2068 | Actions: [260. 256. 343. 352. 282. 575.]\n",
      "Saving model to Pong/, best episode score: 21.0\n",
      "Episode:   1630 | MA50:  +18.7 | Episode Reward:   21.0 | Steps:  2098 | Actions: [299. 335. 287. 355. 287. 535.]\n",
      "Episode:   1640 | MA50:  +18.7 | Episode Reward:   20.0 | Steps:  2096 | Actions: [326. 314. 337. 408. 266. 445.]\n",
      "Episode:   1650 | MA50:  +18.8 | Episode Reward:   20.0 | Steps:  2151 | Actions: [348. 328. 337. 413. 244. 481.]\n",
      "Saving model to Pong/, best episode score: 21.0\n",
      "Episode:   1660 | MA50:  +18.8 | Episode Reward:   21.0 | Steps:  2130 | Actions: [323. 305. 283. 474. 255. 490.]\n",
      "Episode:   1670 | MA50:  +18.6 | Episode Reward:   11.0 | Steps:  2439 | Actions: [357. 332. 319. 553. 326. 552.]\n",
      "Episode:   1680 | MA50:  +18.6 | Episode Reward:   20.0 | Steps:  2068 | Actions: [393. 331. 261. 409. 273. 401.]\n",
      "Episode:   1690 | MA50:  +18.5 | Episode Reward:   13.0 | Steps:  2754 | Actions: [565. 406. 368. 501. 330. 584.]\n",
      "Episode:   1700 | MA50:  +18.8 | Episode Reward:   19.0 | Steps:  2350 | Actions: [441. 319. 307. 566. 298. 419.]\n",
      "Episode:   1710 | MA50:  +18.7 | Episode Reward:   16.0 | Steps:  2337 | Actions: [431. 271. 361. 464. 343. 467.]\n",
      "Episode:   1720 | MA50:  +18.8 | Episode Reward:   20.0 | Steps:  2254 | Actions: [360. 285. 282. 406. 343. 578.]\n",
      "Episode:   1730 | MA50:  +18.8 | Episode Reward:   20.0 | Steps:  2144 | Actions: [369. 344. 269. 375. 265. 522.]\n",
      "Episode:   1740 | MA50:  +18.7 | Episode Reward:   18.0 | Steps:  2487 | Actions: [399. 333. 295. 468. 399. 593.]\n",
      "Episode:   1750 | MA50:  +18.8 | Episode Reward:   19.0 | Steps:  2194 | Actions: [278. 418. 309. 351. 310. 528.]\n",
      "Saving model to Pong/, best episode score: 21.0\n",
      "Episode:   1760 | MA50:  +18.8 | Episode Reward:   21.0 | Steps:  2098 | Actions: [333. 407. 278. 364. 272. 444.]\n",
      "Saving model to Pong/, best episode score: 21.0\n",
      "Episode:   1770 | MA50:  +19.0 | Episode Reward:   21.0 | Steps:  2098 | Actions: [346. 346. 280. 362. 291. 473.]\n",
      "Episode:   1780 | MA50:  +18.9 | Episode Reward:   19.0 | Steps:  2501 | Actions: [413. 351. 424. 407. 331. 575.]\n",
      "Episode:   1790 | MA50:  +19.0 | Episode Reward:   20.0 | Steps:  2128 | Actions: [370. 306. 234. 450. 357. 411.]\n",
      "Episode:   1800 | MA50:  +19.0 | Episode Reward:   20.0 | Steps:  2151 | Actions: [355. 325. 253. 453. 364. 401.]\n",
      "Episode:   1810 | MA50:  +19.0 | Episode Reward:   20.0 | Steps:  2068 | Actions: [359. 262. 241. 521. 337. 348.]\n",
      "Episode:   1820 | MA50:  +19.0 | Episode Reward:   20.0 | Steps:  1991 | Actions: [313. 273. 299. 463. 344. 299.]\n",
      "Saving model to Pong/, best episode score: 21.0\n",
      "Episode:   1830 | MA50:  +19.0 | Episode Reward:   21.0 | Steps:  2098 | Actions: [339. 280. 313. 517. 336. 313.]\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
